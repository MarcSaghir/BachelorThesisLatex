\chapter{Method}
\label{chap:method}


\section{Calibration}
\label{sec:Calibration}
- say that IAD methods only give anomaly score that is not saying anything relevant regarding confidence
- cite  \cite{Wu_2021_shouldbecalibrated} that says that well calibrated ensemble members do not need to yield calibrated output

\section{Discriminator}
\label{sec:discriminator}
Our approach to use a small, compact discriminator to differentiate between regular and anomalous image features is inspired by the approach 
presented in SimpleNet \cite{liu2023simplenet}. Since the discriminators inputs in the ensemble pipeline will be of the same nature as 
the inputs for SimpleNet's discriminator, it is reasonable to utilize their network architeture for this work. Looking back at section 
(simplenet section) and moreso figure xyz(simplenet architecture), we thus will adapt the SimpleNet pipeline after the feature adapter step. 
This means the discriminator, shown as the labelled circle will conceptualy be equal to ours. Instead of the merely adpated features, 
the ensembled features from section (ensemble feature section from methdos) will substitute. The artificial anomalous features, 
depicted as the red tiled pane in the figure will also be provided during training time. Here we also adapt SimpleNet's approach of 
gaussian noise for producing those artificial features. (satz ob wir mit simplex noise arbeiten wenn ja dann erwähnen) As also stated in 
SimpleNet (googlen wie man wörtliche Zitate korrekt benutzt), this discriminator "works as a normality scorer [...] estimating the normality 
at each location (h, w)". Moreover are positive and negative outputs expected for regular and anomalous features respectively.
As to the discriminator network specifics, a regular "two-layer multi-layer perceptron"(zitat markieren) is used. As optimizer a regular 
adam optiizer by pytorch with a learn rate of (werte erst sauber aufschreiben bevor ich es hier hinschreibe)



- say that this is the binary discriminator for detcting the anomalies from ensembled feature maps
- repeat that this is largely based of simplenets discriminator
- describe model architecture as described in simplenet paper
- list parameters from code like optimizer, learn rate, epochs, etc. 
%-> basically die ganze run_ensemble auflisten
- also describe loss


\section{Flat Connector Class}
\label{sec:ourdata}
As previously mentioned in the introduction, this work will also discuss the introduction of three new dataset classes as an addition to the current ones present in the MVTecAD LOCO dataset.
This was to extent the range of objects represented in datasets (referenz auf mvted und loco) and further investigate model performance on industrial manufacturing parts, as this is the 
mein setting for this work. Shaping the dataset in form of the MVTecAD LOCO dataset has multiple advantages. Firstly we get to make statements about the ability of SOTA algorithms detectig 
logical anomalies on industrial parts. Moreover we can easily infer our new datasets with all relevant IAD approaches, since they are nearly all published with MVTecAD benchmarkings, meaning 
they are all released with code to infer on the dataset. As discussed in section (dataset section) the only technical difference between the MVTecAD and LOCO dataset is the storage of the masks, 
which can be accounted for with a few minor changes in the dataset code representation. Since this work also compares AD performances of approaches between both datasets, the functionality 
is already implemented in the linked repository as a result. This makes for uncomplicated inference on the new dataset. Lastly these dataset classes may serve as a base for future benchmarking 
and research of different new IAD approaches. Therefore it is sensible to release the new dataset in the shape of if not the most referenced image anomaly detection dataset(beweis oder umformulieren). 
The three classes are each representing a metal part, namely a flat connector, an angle and ... . For the first to classes, each part that was acquired for the images is available in a 
usual hardware store. The third class was a self crafted composite part made of screws and metal sheets, which were also available to buy at similar stores as the other parts. All of the classes 
meet certain criteria in regards to their material nature, aswell as the possibilities of structural and logical anomalies both occuring with the same part. A solid block of steel for example 
would make a difficult part to represent logical anomalies.
Regarding the recording of images for the dataset, we used (kamera specs) from a birds eye view (nachschauen ob das so heißt) with black cloth (maybe cloth ersetzen und spezifizieren dass es 
dichtes schwarzes material war) as background. The anomalies were handcrafted in the facilities of the university(suchen wie der werkzeugraum heißt und satz neu formulieren). The labels 
were done in the same style as the labels of the MVTecAD LOCO classes, meaning black and white segmentation images, with slightly differing pixel values to match according saturation scores. 

!Subsection mit flat connector!:
For the flat connector we used (maße angeben) regulatory flat connectors (wenn ich lustig bin noch DIN angeben) which are widely available (maybe link referenz). 
Examplary images of anomalous and good images can be seen in figure x. The structural anomalies consisted of damages to the edge of the part, cut off corners and deep scratches on the surface. 
Logical anomalies contained missing holes, additional holes and differently sized holes. For simulating missing holes, the holes were stuffed and then the part was spraypainted wholly. 
Additional holes were simply produced with a drill, likewhise the differently sized holes. 
The corresponding exemplary masks are also seen in fiure x, as an illustration of how the segmentation of the anomalies was held. If compared with the sample images of figure y(mvtedc loco images) 
the similarity is visible. The saturation scores for the anomalies, as discussed in section (dataset section loco) were put at (saturation scores) for all above listed anomalies respectively.



- repeat motivation why we added additional data in mvtec style
- say that we went with loco mvtec flair(maybe give reasons)
- say that we came up with a set of structural and logical anomalies for each category
- list categories(flat connector, angle and special construct)

- 3 sub sections for the three categories

- flat connector
- link the exact one we used(or examples of some)
- give structural anomalies
- give logical anomalies
- for both briefly touch on how we produced them
- show image examples for each

- repeat same for other categories

- also when describing angle:
- touch on how there is a special case with multi perspective detection





\section{pipeline}
- explain brief structure of the pipeline
- ???


\section{Ensemble network}
\label{sec:ourensemblenetwork}
%TO-DO: irgendwo beispielhaft erklären dass manche approaches manche sachen besser verstehen und umgekehrt, deswegen ensemble gut ist, muss maybe in die background oder introduction

There are multiple approaches to ensembling models in general. When combining a heterogeneous set of classifiers, a common approach is to first calibrate(referenz) and then ensemble each models 
output (beweis dafür dass das normal ist). There are also approaches to collectively calibrate a heterogeneous ensemble of classifiers while classifying. While performance varies, combining 
the models is generally not regarded as inherently robust, especially when the classifiers work with features or some other form of representation. This stems from the fact that the model outputs 
do not necessarily reflect their learned representations(neu formulieren) in detail, which in turn means that you cannot obtain the optimal aspects of each part of the ensemble. A more robust 
approach would be to ensemble the mentioned feature maps or other representations to in turn train a discriminator for the final classification. To obtain the different feature representations 
we would use the corresponding training methods of each IAD apporoach and then cut the model of at the respective time. Figures abc show a schematic view of each approachs respective model 
architecture, together with an indication of where the representations would be extracted. Proceeding in this way, we would keep all important features of each representation, resulting in a 
maxmium gain of information and robust predictions over all different classes.
Creating such heterogeneous model ensembles on a feature map level was for instance done in (paper ref). Among other results they investigate the performance of heterogeneous models being 
combined and provide two main approaches to doing so:
\textbf{General Transformation Block}




- talk about different ensemble approaches we discussed: ensemble model outputs and ensemble model feature maps

feature ensemble:
- ground idea: have different algos extract features, and then ensemble them. Afterwards train discriminator on the ensembled features like in simplenet
- reference paper that quses PCA and global block transformation
- global transformation block:
-> resize all feature maps to same dimensions
-> append feature maps
-> PCA: keep either percentage or set amount

- individual transformation block:
-> first apply PCA
-> sagen wann das am besten anwendbar ist, auch sagen dass für uns probably der global transformation block reicht
-> dann zusammenführen mit resize und appnden


%brauche ich die section??
\section{Different ensemble approaches}
- weighted, random forest etc 
- specifics


%neu formulieren das kling scheiße 
\section{Logical Anomaly Detection Using Conventional Approaches}
As discussed in the introduction section(\ref{chap:introduction}), logical anomalies represent a signifacant part of image anomaly detection in modern
manufacturing settings. The experiments also serve as an extensive comparison of SOTA methods for IAD versus recent approches that where 
introduced with special mind to logical anomalies, like GCAD \cite{LOCODentsAndScratchesBergmann2022}(GCAD reference von Paul Bergmann). 
Moreover, for a qualitative evaluation of the performance change when using feature level ensembles, one first needs to evaluate the base performance 
of each relevant classifier of the set. 
Hence this work features experiments to evaluate IAD approaches mainly evaluated on the classical MVTecAD dataset. To do so, the original 
code from each paper was taken and not modified in regards to any reportet parameters and/or arguments. This was to prevent possible unwanted deviations 
in original performance by changing up synergies. This paper recognizes the possibility of improved performances on the logical anomalies dataset 
with different combinations of model parameters. Yet this work focusses on the performance(synonym) of current unmodified apporoaches and 
more importantly the increased robustness through the use of ensembles. Therefore research regarding this hypothetical improvement would 
have to be done in another work. Metrics that are specifically looked at in this context are the AUROC, pixel AUROC(weitere maybe einfpgen) and the sPRO. 
If the functionality to evaluate these metrics was already given, the results of inference were (übernommen), else the according functionality 
was implemented in this work and used to produce the according metrics. 
Papers whose approaches were evaluated using the MVTecAD LOCO dataset were: SimpleNet \cite{liu2023simplenet}, PatchCore \cite{patchCore2022} (list of paper references with names). 
These papers were discussed in depth in the backgrounds section and any specifics like 
hyperparameters can be viewed in the corresponding paper. Furthermore all named classifiers were including, among other variable measures, 
a preprocessing step to resize the input image. This makes for a variable model input and also the ability to process rectangular images, 
which is important due to MVTecAD LOCO images being rectangular unlike the squared input from the standard MVTecAD dataset. The only 
necessary modification to the whole process of anomaly detection was the generation of image masks. The MVTecAD LOCO dataset stores its 
masks in multiple seperate black and white images, one for each individual anomaly. To fix errors stemming from this fact, additional 
code was added that pastes all masks belonging to one image into a single mask before iterating through the data.


Überschrift reformulieren!

What i wanna say in this section:
- what we did to do the survey on LOCO IAD detection
- what we did to the methods(nothing)
- aspekte anhand welcher wir die experimente analyiert haben

- wenn ich es actually auch mache dann ablation experiments nennen in welchen ich die images square






