\chapter{Experiments}
\label{chap:experiments}

In this chapter the experiments and experimental results of this work are displayed. First we establish the general experimental setup. Afterwards the results of the conducted MVTecAD LOCO \cite{LOCODentsAndScratchesBergmann2022}
survey are presented in section \ref{sec:locoxperiments}. As a baseline for performance evaluation we uitilize the performance on the more conventional MVTecAD dataset \cite{MVTEC_Bergmann_2021}, 
as well as a classifier comparison. In section \ref{sec:faltconnectorxperiments} we review the performance of the classifiers on our novel dataset category. Lastly section \ref{sec:ensembleresults} 
deals with the findings regarding the ensemble network approach, also conducted on the MVTecAD LOCO dataset and the new dataset category introduced in this work.


\section{Experimental Setup}
\label{sec:experimentsetup}

All models trainings and result reproductions have been conducted on the IAD cluster student partition. The GPU in use for all nodes used by that partition is an 
RTX 2080Ti with 11GB of memory, and the CPU is an AMD Ryzen 9 16-Core processor. The cluster overview \cite{clusterdocs} serves to provide further detail for additional questions. 
As for software, the specifications of other libraries, as well as the specifications for the MVTecAD LOCO experiment are documented in the 
environment files in the project code.



\section{MVTecAD LOCO Experiments}
\label{sec:locoxperiments}

In this section we review the performance of the IAD methods mentioned in section \ref{sec:IADmethods} on the MVTecAD LOCO \cite{LOCODentsAndScratchesBergmann2022} 
dataset. Tables xyz, xyz, and xyz each display the experiments with regards to one of the metrics: image AUROC, pixel AUROC and sPRO. \newline
Comparing the average image and pixel AUROCs to the ones reported in the chapter \ref{chap:background} a decrease in performance is visible. 
This is also true in regards to the pixel wise AUROC on average. (noch hinschreiben welche classifier den maximalen drop off hatten und 
welche den wenigsten hatten). (Schreiben Welche classifier on average die besten waren für jede metric)

\input{algotable/locoimageauroc.tex}

Investigating the column wise performance, the classes of the dataset seem to have varying challenge levels. Albeit some fluctuations, 
the juice bottle class seems to be consistently getting very high results in image and pixel AUROC, especially regarding the latter. 
Conversely, the class screw bag seems to often have lower image AUROC resuts.

\input{algotable/locopixelauroc.tex}


The sPRO values calculated for the approaches were of poor performance as seen in table xyz. Despite this, comparing the metrics to the 
class averages reportet in \cite{LOCODentsAndScratchesBergmann2022}, the results seem legitimate. Bergmann et al. review several methods 
in this paper that they selected to be applicable on logical anomalies. The lowest scoring models were often barely above some metrics reportet 
below, if at all. (Beschreiben wer so gute ansätze sind bei sPRO on average)

\input{algotable/locospro.tex}


To bring the above mentioned numbers into perspective, the drop of in image and pixel level auroc for the classififers between the 
MVTecAD \cite{MVTEC_Bergmann_2021} and the MVTecAD LOCO \cite{LOCODentsAndScratchesBergmann2022} dataset was significant. Approaches 
PatchCore \cite{patchCore2022}, SimpleNet \cite{liu2023simplenet}, Reverse Distillation \cite{revdist2023} and DRAEM \cite{Zavrtanik_2021DRAEM} had a fall off 
in image AUROC class average of respectively 0.174, 0.193, 0.208 and 0.213. With respect to the pixel wise 
AUROC, a decline of 0.064, 0.147, 0.218 and 0.174 emerged. \newline
Figure xyz shows exemplary metrics divided by structural and logical anomalies. The reported pixel AUROCs slightly differ from earlier reported results due to a different 
evaluation split of good images, but showcase a principle performance difference. Here a better performance is clearly visible for the 
class of structural anomalies, sometimes by even up to 29 percent, suggesting logical anomalies to be distinctively more difficult to correctly segment. (Prediction plot???)

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/structvslogic.png}
    \caption{Exemplary pixel AUROC comparison of PatchCore \cite{patchCore2022} between structural and logical anomalies. For each evaluation class all normal class images 
             are taken into calculation, resulting into deviating AUROCS from the original reported ones.}
    \label{fig:structvslogic}
\end{figure}

Additionally below in figure xyz are representative images from multiple classes that showcase good and bad segmentation results. Results are taken from multiple approaches and 
more results can be found in the appendix.

\input{figures/locopatchcoreresults/locopatchcoreresultsBB.tex}
\input{figures/locosimplenetresults/simplenetJBresults.tex}


\subsection{Classifier Behavior}
\label{subsec:classifierbehavior}

When inspecting localization performance in segmentated images, some classifiers exhibited different strengths and weaknesses than others. 
DRAEM \cite{Zavrtanik_2021DRAEM} for instance often showed very precise localization performance in multiple classes, but struggled 
with correctly segmenting anomalies in cases where objects are missing and there is not visbile border on where this object should 
appear. For instance, precisely localizing missin screws in the right compratment of the braekfast box class was done well, yet 
segmenting a missing screw that could be anywhere in the screw bag was often barely done at all (Fig. \ref{fig:DRAEMweakness}). As these logical anomalies ocurr 
frequently in the MVTecAD LOCO \cite{LOCODentsAndScratchesBergmann2022} dataset, it also explains how DRAEM 
has the largest drop in average performance among the classifiers, despite its mostly precise localization. Another aspect to this result is DRAEM being very strict with 
segmentations. often times results are perfectly interpretable by humans, but may perform worse than other segmentations, as DRAEM does not necessarily segment all pixels in 
an anomalous region.

\input{figures/DRAEMweakness/draemweakness.tex}

The other classifiers, PatchCore \cite{patchCore2022}, SimpleNet \cite{liu2023simplenet} and Reverse Distillation \cite{revdist2023}, generally showed good performance. Unlike DRAEM, they exhibited 
poorer performance when classifying small anomalies (Fig. xyz). Moreover The segmentations often times slightly overshoot the anomalous regions at various confidence 
degrees. What this means is that unlike DRAEM \cite{Zavrtanik_2021DRAEM}, which is very strict with segmentation, those approaches tend to segment more pixels than necessary. 
Of course these pixels do not have the same values, which is why the performance is still good after thresholding. 

- figure mit patchcore und simplenet results



\section{Flat Connector Experiments}
\label{sec:faltconnectorxperiments}

The experiments on the flat connector class produced high performance from all classifiers participating in the survey. Table \ref{tab:flatconnectorperformance} 
displays an overview of important metrics calculated on this novel class.

\input{algotable/flatconnectorperformance.tex}

The localization performances display similar characteristics of each classifier as during the last experiment. This relates to DRAEMs \cite{Zavrtanik_2021DRAEM} precise segmentation, 
aswell to the more fuzzy segmentation of the other classifiers. Below are representative results of the classifiers localization performance on the flat connector class (Fig. \ref{fig:FCallapproaches}).


\input{figures/allapproachesFCimages/allapproachesFC.tex}



\section{Ensemble Network}
\label{sec:ensembleresults}

This section reports the results of the ensemble network approach on the class flat connector to facilitate understanding of a visual 
analyis. Furthermore, as the flat connector class seems to be less of a challenge to this works approaches than other classes, it is convenient to showcase strenghts and 
shortcomings of the ensemble models in a more easy environment. More experiments from other classes of the MVTecAD LOCO \cite{LOCODentsAndScratchesBergmann2022} set are to be found in appendix (referenz). 
Conclusions drawn from this 
experiment are also applicable to the other classes. Firstly the results from the primary ensemble approach from section \ref{sec:featurelevelensemble} are reported. Afterwards 
the results of the secondary ensemble approach are presented, supported by according metrics and segmentation examples.


\subsection{Independent Transformation Block}
\label{subsec:ITBfail}

When performing the ensemble training process using the approach by \cite{EnsembleHeller2023}, the results were generally not usable. Looking at the exemplary plot of the segmentation 
results in figure \ref{fig:pca_res}, it becomes obvious why no metrics are reported for this experiment. When investigating image level metrics, they were highly inconsistent thorughout 
the training process and are regarded as not meaningful and representative.

\input{figures/pca_results/pca_results_figure.tex}

As visible in figure \ref{fig:pca_res}, the segmentation results appeared to be not more meaningful than gaussian noise, strongly suggesting that this method has failed. Subsection 
\ref{subsec:ITBfaildiscussion} will go into possible reasons for the suboptimal experiments results. Concludingly it is to be said that these results are, besides the failure analysis 
in the conclusion, not relevant and will not be part of further review. Thus no results of other classes using this method are posted in the appendix.

\subsection{Stacking Ensemble}
\label{subsec:stacking}
This subsection reports the performance of both ensemble experiments with stacking as the ensemble method. 
The experiment is split into two smaller experiments. One investigates the performance of ensembling higher and lower level features in 
single backbones, wheras the other one regards standard backbone ensembling with the ones mentioned in section \ref{sec:ensemblecandidates}. 

\input{algotable/ensembleimageAUROC.tex}

\input{algotable/ensemblepixelAUROC.tex}


\textbf{Hierarchy Levels.} The results of ensembling different hierarchies exhibit a degree of diversity between different layer hierarchies. As briefly menitoned in section \ref{sec:ensemblecandidates}, the resnet backbones consist of four 
layers, in other words they contain four larger sequential blocks. Inference using higher level feature representations with an backbone of subtype resnet resulted in unusable results, segmentation wise. 
Figure xyz exemplary showcases the segmentation results of training using the wideresnet50 backbone and including a feature aggregation between layers 3 and 4. 

\input{figures/faillayer34/faillayersegmentations.tex}

The image and pixel AUROC were unstable and poor over the course of the training, 
as figure \ref{fig:failmetrics} suggests, and therefore were not recorded explicitly for multiple classes, as this was the case for all trainings. The loss visible in figure \ref{fig:failmetricsloss} furthermore 
showcases that the training process was flawed. Due to these findings the following experiments only consider earlier layers if diverging from the standard layers 
discussed in section \ref{sec:ensemblecandidates}.

\input{figures/faillayer34/failmetricsandloss.tex}


Experiments combining lower layer aggregations yielded different results. Table xyz summarises the reported metrics on the MVTecAD LOCO \cite{LOCODentsAndScratchesBergmann2022} 
dataset, and figure xyz showcases exemplary 
segmentation images of the experiments. More data is to be found in the appendix.

\input{figures/ensemblehierarchyimages/ensemblehierarchy.tex}

Here it is to be seen that the results are usable, as anomalies are segmented. The localization performance seems to be worse than the standard SimpleNet \cite{liu2023simplenet} 
performance, judging from segmentation images (Fig. \ref{fig:ensemblehierarchy}) and metrics (Tab. \ref{tab:ensembleimageAUROC}, Tab. \ref{tab:ensemblepixelAUROC}). While the anomalous regions often still get detected, there is a lot more uncertainty regarding 
the pixel scores than from comparable IAD approaches, leading to less robust results.



\textbf{Backbone Ensemble.} The performance of this approach is visible in table xyz. The performance is visible worse from the performance 
reported by the standard simplenet approach. Yet the results suggest usable results unlike the experiment conducted in section \ref{subsec:ITBfail}. 
Moreover the loss, presented in figure xyz, suggests a conceptually adequate learning. The performance could not be increased by a higher 
epoch count as to be seen by the loss progression in the other subfigure.

\input{figures/ensemble_losses/ensemble_loss.tex}

Moreover figure xyz showcases exemplary segmentation results on the flat connector class. As 
seen the ensemble ouputs consist of a lot more uncertainty, as the values are closer together and visually harder to distinguish This uncertainty 
was higher for logical anomalies than for structural ones. Still, 
using threshoolding the results are usable, as often anomalous regions are discernible. Figure xyz also showcases examples of failed segmentation 
attempts. 

\input{figures/ensembleimagesFC/ensembleimagesFC.tex}

As a comparison figure xyz displays example segmentations from the individual backbones. Here there is a lot less uncertainty or 
noise than in the ensembled approach. This makes for worse and less robust ensemble performance, especially regarding segmentation. 
The implications and reasons of this are taken up in the next chapter.

\input{figures/backboneexamples/singlebackboneexamples.tex}




