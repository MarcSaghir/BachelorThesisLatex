\chapter{Experimental Results}
\label{chap:experiments}



- analysis on how methods worked on own dataset individually
-> if poor performance error analysis and also address different subclasses

- analysis of how ensemble model worked and if it improved performance



\section{SOTA Methods Performance on classical LOCO Dataset}
\label{sec:locoresultssota}
In this section we review the performance of prior introduced anomaly detection methods. All experiments were performed with the same 
experimental setup as explained in section (referenz of experimental setup section), the conditions explained in section (referenz von methods section über loco) 
and on the mvtec LOCO dataset \cite{LOCODentsAndScratchesBergmann2022}. 
The results of inference on the test set can be seen in table x (tabelle mit ergebnissen). As it can be seen, all models scored a significantly 
lower result on the MVTecAD LOCO dataset than on the normal MVTecAD one(exemplary scores seen in table xy(table mit normalen mvtec scores)). 
A lower performance is generally to be expected, since firstly logial anomalies are regarded as a more difficult problem than structual 
ones and secondly the average SOTA performances as seen in table x(tabelle mit ergebnissen) is already closing in on an AUROC of of 1. 
(den satz rechts von hier müsste man maybe rausmachen oder umschreiben)Therefore there is not much room for further improvement in similar settings, and a worse performance still aknowledgeable as very good. 
Yet there is an drop in cross-model average AUROC of approcimately (durchschnitts drop ausrechnen), which is a remarkable(synonym) difference. 
Most other metrics, namely (metrics names), also declined with an respective average of (respective averages). As explained in section 
(referenz zu metrics section von background), the sPRO (or rather AU-sPRO) was a score introduced in \cite{LOCODentsAndScratchesBergmann2022} to gain an 
advanced insight on the quality of segmentations. This means that all approaches who either were published before or did not include this 
paper in their research likely did not include this metric, which holds true for the approahces used for this experiment. Therefore no comparison 
in sPRO/AU-sPRO can be shown(vllt einfach spro auch für allte ansätze implementieren?? dann kann ich den satz ändern). Comparing the sPRO 
scores of the SOTA methods in this experiment with the ones from compared to GCAD \cite{LOCODentsAndScratchesBergmann2022} shows asignificantly 
(abchecken ob wirklich) worse performance.
Among the different models, the highest scoring one was PatchCore \cite{patchCore2022}. It scored an average (metrics einfügen) feature embedding based approaches like  
achieved the highest scoring

Interpretation of results hier, weiß nicht in welche section das eigentlich muss:



\section{Ensemble Performance}
Notizen für diese section:
- hier soll reportet werden wie das ensemble sich geschlagen hat
- dazu brauche ich:
-> metriken(AUROC sPRO und vllt pixel auroc) von dem ensemble auf flat connector + mvtec loco
-> beispielhafte segmentierungen
-> plots von loss und auroc über training

- drauf eingehen wo sich das ensemble wie gut geschlagen hat
-> vergleich mit patchcore und simplenet wichtig, gerne auch mit DRAEM vergleichen als reconstruction representativer algo
-> sagen bei welchen klassen es gut und nicht so gut geklappt hat, vergleichen mit ergebnissen aus LOCO studie oben drüber(vllt in conclusion?)
-> mehr images in appendix anbieten





