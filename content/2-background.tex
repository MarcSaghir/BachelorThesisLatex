\chapter{Background}
\label{chap:background}



\section{Ensembles}
\label{sec:ensembles}
%TO-DO: irgendwo beispielhaft erklären dass manche approaches manche sachen besser verstehen und umgekehrt, deswegen ensemble gut ist, muss maybe in die background oder introduction

When it comes to ensembling classification models, there are multiple approaches to do so. Many ensembling methods are focussed on combining homogeneous 
models, meaning a set of related models with similar architecture but different parameters or initializations. Typical methods include 
(liste an methods mit referenzen, majority vote, boosting, bagging, stacking??, CAWPE, blabla), of which i.e. (ansatz für bäume ensembles) are a typical approach to boost simple 
classifiers like trees. Homogeneous ensembles are popular, since they tend to boost the performance and robustness of a base classifier without lots 
of additional work, since the ensemble is normally created by initializing the models in different ways.
%kann der teil über diesem kommentar in die introduction?
Heterogeneous classifier ensembles on the other hand are not necessarily combinable that easily, since they usually consist of models with 
different network architectures. This can lead to results, that should be interpreted as the same, differing by large margins(synonym). Yet 
ensembles of such variety are often desirable since they offer loads of information from different perspectives or domains when done right. 
% Kann der satz weg? Combining knowledge from different domains with such an ensemble is an effective way to improve a models performance in foreign domains.
Thus to bridge this gap at the output, a common approach is to first calibrate(referenz) and then ensemble each models 
output (beweis dafür dass das normal ist). For the last combination step, all ensemble techniques suited for homogeneous ensembling can 
be applied, due to the outputs being in a comparable state then. There are also approaches to collectively calibrate the hyperparameters 
of each heterogeneous classifier while classifying(referenz). While performance varies, combining 
these models in such a way is not necessarily regarded as the highest achievable robustness, especially when the classifiers work with features or some other form of inner representation. 
This stems from the fact that the model outputs are merely a small result of larger inner representations that may focus different aspects 
of information among the inputs. Therefore in turn, you cannot obtain all relevant information that can be offered by simply calibrating the 
model outputs. A more robust approach to address that problem, would be to ensemble the aforementioned inner representations, i.e. feature maps 
and in turn train another classifier for the final meaningful output.
%----nachfolgender satz maybe raus mal gucken oder anders einbauen
Another limitation of both kinds of ensembles, being homogeneous and heterogeneous, is that all models have to actually be trained 
seperately to then utilize the different classification ouputs. This leads to a highger training time and thus also higher computational 
cost, which is desirable to be reduced in real world manufacturing firms.
%-------
The robustness and efficiency has been demonstrated in \cite{EnsembleHeller2023} (ensemble referenz). The authors utilize a feature level ensemble of multiple 
convolutional neural networks with different architectures and tasks to improve inference speed and accuracy in plant disease detection.
(Heller referenz) show that cutting off several, potentially heterogeneous, classifiers after a couple of network layers and ensembling the 
resulting feature maps yields firstly a significant improvement in training time compared to classical output ensembles. This stems from 
the fact, that all base classifiers of the ensemble only have to be trained once for every following training approach. During this the 
model still stays compact(zitat aus ensemble paper markieren), giving it an advantage over most supervised approaches(satz klingt scheiße)(irgendwas mit 
lightweight hinschreiben? -> nochmal paper gucken). Moreover they compared the performance of different ensemble combinations with conventional 
output ensembles via softmax and reported in all cases no significant drop in performance. In cases where this approach allowed for 
different inputs via multispectral cameras(zitat markieren) there even was a similar performance of this ensemble to other state of the 
art ensembles visible. Keeping in mind the compactness of this new ensemble model combined with an equal performance and possible 
increased robustness, as argued prior, it is a promising ensemble approach for this work.
To obtain ensembled feature maps the paper proposes to bring all feature maps to the same sizes using bilinear interpolation. Since 
it is not desirable to keep every available feature map, as this would create inputs with way too many features, the amount of feature 
maps is reduced using principle component analysis. This allows for the ensemble to focus only on the most important features, while maintaining 
an equal amount of maps as if it were composed of a single classifier. To be more specific (Heller+co) \cite{EnsembleHeller2023} introduced two different 
approaches to perform this ensemble. The first is a global transformation block as seen in figure xyz(figure mit global transformation block). 
Here the features are first all resized to the same dimensions and then connected along their channels through a concatenation layer.
Afterwards PCA is applied along the channel dimension to obtain a result with N remaining feature maps, where N can be adjusted for ones 
needs.



- soll ich die transformation blocks erst bei den methods genauer beschreiben?

- hier vorgehensweise und findings von ensemble paper schreiben
- erster ansatz für vorgehensweise:
The actual combination of features from different level 1 classif


%part below maybe in methods
To obtain the different feature representations 
we would use the corresponding training methods of each IAD apporoach and then cut the model of at the respective time. Figures abc show a schematic view of each approachs respective model 
architecture, together with an indication of where the representations would be extracted. Proceeding in this way, we would keep all important features of each representation, resulting in a 
maxmium gain of information and robust predictions over all different classes.
Creating such heterogeneous model ensembles on a feature map level was for instance done in (paper ref). Among other results they investigate the performance of heterogeneous models being 
combined and provide two main approaches to doing so:
\textbf{General Transformation Block}




- talk about different ensemble approaches we discussed: ensemble model outputs and ensemble model feature maps

feature ensemble:
- ground idea: have different algos extract features, and then ensemble them. Afterwards train discriminator on the ensembled features like in simplenet
- reference paper that quses PCA and global block transformation
- global transformation block:
-> resize all feature maps to same dimensions
-> append feature maps
-> PCA: keep either percentage or set amount

- individual transformation block:
-> first apply PCA
-> sagen wann das am besten anwendbar ist, auch sagen dass für uns probably der global transformation block reicht
-> dann zusammenführen mit resize und appnden



\section{Categorization of Anomaly detection}
\label{sec:IADcategs}
When trying to understand the choices of IAD approaches for the pipeline and ensemble, one first has to learn about a few important distinctions of models on this topic.
The deep learning approaches that have established themselves as state of the art in image anomaly detection are almost exclusively unsupervised approaches. This partiall stems from the fact 
that naturally anomalous images occurr far less than normal images, hence the word "normal". This is especially true in industrial settings, due to the high performance of production factories 
nowadays. Therefore if one were to consider using a supervised learning approach to detect anomalies, either a strong class imbalance or an unrepresentative class distribution would occur.
While there are some solutions for this, they often are either not goo enough for imbalances this high(synonym klänge cool) or far to extensive. Some papers like (supervised papers zitieren)
utilize supervised approaches with some success, but still yield a worse performance than the popular unsupervised approaches generally used. Consequently the biggest model distinction is 
between unsupervised and supervised ones. Here it has to be said that there are technically also other settings of IAD one could talk about at this level of observation, but since we are also 
directing our focus to to RGB images, they will not be talked about. Moreover one has to make some simplifications to allow such sharp categorizations of partially interwoven approaches.

The supervised learning category could also further be split up into sub-categories at a lower level. But seeing as the performances of unsupervised approaches dominantely outweigh the 
performance and cost of the former, this work will solely focus on the latter kind of approaches. In the unsupervised IAD setting we then normally distinguish between reconstruction and 
representation based models. One of the key differences between those two is(hier dringend auch paper zitieren die das untersuchen), 


... 


If we now consider the classification of algorithms above, aswell as figure x, we can see that there are quite a lot of unique models and approaches to the same end. To ensure that the built 
pipeline is able to help experiment on images from different points of view, so to say, aswell as ensure that our ensemble approaches cover as various different aspects as possible, it is 
crucial to select approaches from majorly different branches. Here it may be noted that the performance of the single models is not completely disregarded, as those models may prove themselves 
not very useful in the ensemble setting or even as a point of view for experimentation. Therefore certain approaches from the survey papers ...., which yielded performances that were not 
remotely comparable with the highest performing models, were not considered, even if they might cover a previously unrepresented class of IAD setting. 
The main choices were:
- patchcore + paper
- DRAEM + paper
- CSFlow + paper

With this choice we still represent reconstruction and representation based settings somewhat comparably, aswell as providing different examples for a variety of subclasses, namely
distribution maps, autoencoder, memory banks, teacher-student models, diffusion models and ...



- there are different kinds of approaches to IAD
- look at tree picture
 
- First important distinction is between supervised and unsupervised
-> we focus on unsupervised
-> list problems with supervised approaches and thus advantages of unsupervised ones

- briefly touch on other IAD settings like few shot, along with references

- among unsupervised approaches, there are two more fundamental distinctions
-> reconstruction based vs representation/feature embedding based
-> explain difference with lots of references

- for reconstruction based touch on 2-3 base categories like GANs etc and link fundamental papers for GANs etc
- for representation based important to explain memory bank, teacher student, and distribution map
- explain normalizing flow somehow somewhere in there

- maybe say which algos we chose and what we covered with that



\section{Datasets}
\label{sec:datasets}
The datasets used in image anomaly detection are scarce, especially when it comes to anomaly detection in a manufacturing setting. There are many datsets and approaches that specialize on certain materials \cite{FabricDataset_Tsang_2016} 
\cite{SteeltubeDataset_Yang_2021} \cite{magnetictiles_Huang_2018}
and often only one classs. What currently stands out as a gold standard among IAD datasets is the MVTecAD \cite{MVTEC_Bergmann_2021} dataset. The authors created it  
as a highly representative and standardized set of anomalous images along with training images. It has 15 classes from capsules to screws. Moreover the dataset provides image labels aswell as segmentation 
ground truths, making it versatile and applicable for multiple algorithms. The masks come as black and white grayscale images, while the image labels are given through its folder structure. 
Its paradigmatic structure tree can be seen in figure xy. As shown, each class contain train images, which only consist of regular examples, 
and test images. The data among the testing images is categorized by a title describing the anomaly. The ground truth folder contains 
according ground truths on a pixel level. Example images of the dataset are to be seen in figure z. They typically are of a rectangular shape and their resolutions range from 
(pixel min) to (pixel max). More specifications can be found in Bergmann et al. \cite{MVTEC_Bergmann_2021} and the whole dataset is publicly available at the official website\cite{mvtecdownload}.\newline
The MVTecAD(referenz) dataset is regarded highly among IAD papers, and has since its introduction been used in most relevant papers as a dataaset 
to benchmark the respective approaches on. This is also likely to remain the trend, since many state of the art algorithms in the recent years have primarily been benmarked on it, forcing new approaches 
to also be benchmarked on this dataset to be comparable to the current highest performance holding approaches. Despite this work focussing on manufacturing settings MVTecAD is one of only two datasets relevant to this work, 
and serves as a comparison for the performance investigation of this papers approaches on the second dataset. This is mainly due to the dataset's importance and 
its relation to the second dataset.
\newline
Later in 2022 Bergman et al. has introduced another IAD dataset that is loosely related to their original MVTecAD dataset, namely the MVTecAD LOCO dataset \cite{LOCODentsAndScratchesBergmann2022}. 
This dataset works with the same ground ideas as their original MVTecAD set, but extends the conceptual contents of the dataset by logical anomalies(neu formulieren das klingt scheiße). 
It consists of five classes: breakfast box, juice bottle, pushpins, screw bag and splicing connectors. The difference to the other dataset is that the anomalous categories for each class are only seperated into good images, images with structural anomalies 
and images with logical anomalies. As mentioned in the introduction structural anomalies are visible damages to the objects, similar to the MVTecAD dataset. Logical anomalies denote violations against arbitrary restrictions 
imposed by the authors. To illustrate this by an example: The class of pushpins represents a birds view of a compartmentised box of pushpins(see figure a). A rule added was, 
that each compartment is only to contain one pushpin. This means that if one region were to miss their contents, or contain more than one pushpin, it would constitute a logical anomaly. If on the 
other hand a pushpin would have a crooked or broken tip, it would be  labelled a structural anomaly. Structurally the differences of the 
MVTecAD and the MVTecAD LOCO dataset can be seen when comparing figures a and b, which showcases the anomaly classification, aswell as 
the method of storing segmentations. Here there eixists an image file for each anomalous ground truth area, which are mapped to the image 
by the folder name they are in. Lastly there exists a validation set in this dataset,
\newline
The addition of logical constraints opened an interesting area of research, since the high performance 
of current state of the art algorithms were only measured on structural anomalies so far. Yet it would be insightful to see if those models could also detect logical anomalies, since those also ocurr 
in real life settings, such as manufacturing settings. Another concept introduced in \cite{LOCODentsAndScratchesBergmann2022} is the 
saturated per-region overlap score, also sPRO. The metric is further analysed in setion (metrics section), but in short gives a measure 
on how well two regions overlap, while also accounting for regions overlapping in a way, that is seen as sufficient. The criterion of 
sufficiency is given by a file in the respective class, which maps a saturation score to each kind of anomaly.
Bergmann et al.\cite{LOCODentsAndScratchesBergmann2022} lastly also released a new IAD model together with the new dataset. The model uses autoencoders(bissi besser beschreiben hier). Since the source code has not been made public, 
this work refrains from using the method proposed in the paper.




\section{Metrics}
\label{sec:metrics}
Metrics are known to be an important part of developing any artificial intelligence related models. Many of them are used to infer 
different characteristics of model performance and should be used in different appropriate circumstances, depending on which aspect 
is important for the current application. Therefore, before the actual developing, one must first choose appropriate metrics 
to optimize and evaluate on later. IAD as a research area themselves has certain metrics that are the main performance evaluation tool 
across most papers. 
A collection of different metrics in this domain are displayed in table \ref{tab:metrics}, which is taken from 
\cite{liu2024deep}. Visible are well known 
ones from many other machine learning models like precision, recall, TPR, FPR and the F1-Score. These are generally applicable in most 
cases, but are not listed in any recent important papers and thus are not important for any analyses in this work. The other metrics are 
more IAD specific. By a large margin, the most important scoring standard is the AUROC. This metric is usually referenced for image level 
binary classification and gives an indication on how good the model is able to distinguish between both classes. Its calculation can be seein in 
table \ref{tab:metrics}. Moreover it can be used on a pixel level, which is also a popular approach but not utilized everywhere.
Next in importance is the per-region overlap(PRO) score or also the area under the PRO score(AU-PRO). This metric denotes the per-region overlap of two areas 
on a pixel level and can be calculated using (PRO formel hier hin, im satz rechts dann ggf bezug auf formel zeichen nehmen). The two areas 
compared are generally an image mask and the according segmentation by the model. The AU-PRO is then calculated by plotting the PRO score 
at different threholds for the segmentations, and reporting the area under the curve. This can be 
used to rate the segmentation performance of different models and is also a frequently featured metric in IAD related research. 
Related to this score is the saturated per-region overlap (sPRO) and also the according are under the curve, the AU-sPRO. This metric was 
introduced in \cite{LOCODentsAndScratchesBergmann2022} and briefly mentioned in section (dataset section). The method of deriving the sPRO 
score is shown again in equation abc, where $m$ denotes the amount of anomalous regions in an image, $A_i | i \in \{1, ... , m\}$ an anomalous 
region among them and $s_i | i \in \{1, ... , m\}$ a respective saturation threshold. $P$ is considered to be the pixels classified as anomalous 
in the target image. The sPRO score is the calculated by averaging the intersections of all predictions and ground truths of an image, 
while norming the values by the saturation threshold and proividing an upper limit of 1 per region. It is to be said that this gives a similar 
view on the segmentation performance as the PRO score, as it is a generalized form of it and can producet the same results if the saturation 
threshold would be equal to the amount of anomalous pixels per region. However, due to its cap of 1, it also rates differently large segmentations equally in cases 
where the anomalous position posesses some uncertainty. Figure xy demonstrates this behaviour in the case of a logical anomaly of the pushpin class. 
As visible, the logical anomaly consists of an empty pushpin compartment. The missing pushpin could be placed in any place of this smaller 
box for it to be valid, therefore an amount of pixels equal to the amount a visual pushpin posesses would suffice. Yet the conventional PRO score 
would keep on rising as the segmented area gets larger within the anomalous region. Due to the saturation score and limit. this is prevented 
by the sPRO metric as figure xy shows it to be already saturated once the minimum required amount of pixels is achieved. The saturation 
scores for each anomaly have to be individually set for each anomaly, and are given for the five classes of MVTecAD LOCO \cite{LOCODentsAndScratchesBergmann2022}.



\section{Anomaly Detection Methods}
\label{sec:IADmethods}
- hier noch auflisten welche algos wir durchgehen


\subsection{PatchCore}
\label{subsec:patchcore}
Notizen zu erklärung von patchcore/dinge die in diesem abschnitt stehen sollen:
- PatchCore it ein representation based approach -> auf einteilung in section zu IAD methods klassen verweisen
- noch spezifischer ein memory bank one

- vllt nochmal recap: was genau memory bank bedeutet

- anhand von figure das vorgehen der patchcore pipeline erläutern:
-> normale images werden bei training gegeben
-> mit pretrained backbones werden features extrahiert und zu locally aware patch features, aka patches unmgewandelt -> sagen was das heißt
-> 1-2 Formeln aus patchcore übernehmen für patch erklärung damit alles mathematischer aussieht
-> hinzufügen der patches zu memory bank
-> das coreset/die bank subsampling danach erwähnen, passiert glaube ich aus effizienzgründen

-> zur inference wird input bild auch zu patches verarbeitet genauso wie training images
-> nearest neighbor search für nächste patches, jeder patch kriegt eine distance
-> anhand von distanzen werden anomaly scores ermittet wobei der größte score unter den patches gleich dem anomaly score ist
-> Formel für distanzen aus patchcore zeigen und erklären
-> segmentations auch anhand von distances durch interpolation und smoothing

- Leistung von patchcore:
-> super gut, mitunter bester ansatz sowohl detection as auch localization, AUROC, PRO und alles von patchcore reporten und auf example segmentations verweisen
-> geschwindigkeit: ???(probably sehr gut, subsampling beschleunigt NN suche)
-> geringe storage cost wegen subsampling


- Limitations: patchcore limited by pretrained extraction backbones -> sollte ich auch noch bie introduction zu den problems schreiben oder bei background zu representation dingsies


\subsection{SimpleNet}
\label{subsec:simplenet}
Notizen für simpenet/sachen die ich sagen will:

- simplenet ist auch representation based aber ein feature projection approach(factchecken!!)
- erklären was das heißt, zb dass die features in eine bestimmte dimension projected werden um aussagekräftiger zu sein
- published as easy to use and application friendly approach by blabla

- simplenet pipeline:
-> training:
-> features werden auch hier mit pretrained backbones extracted und wie bei patchcore zu locally aware patches gemacht
-> features werden projected wobei die projection auch erlernt wird(sagen dass single layer network genügt hierfür)
-> fake features werden generiert. simplenet disst hier die bisherigen ansätze von synthetischer data(referenzen) und schwört auf gaussian noise
-> fake features sind dann richtige features + gaussian noise
-> discriminator: 2 layer MLP, kept very simple
-> features are appended and all batch is fed to discriminator per epoch
-> discriminator then learns on them
-> loss beschreiben auch mit formel
-> discriminator spuckt werte aus(scoring formel zeigen)
-> anomaly map ist discriminator pro patch, hochinterpoliert, image score ist höchster map score

Leistung von simpenet:
- sehr gute results, auroc und pro bitte raussuchen
- geschwindigkeit vergleichsweise langsamer als zb patchcore und padim welche representation based sind aber auch langsamer als reconstruction 
based DRAEM -> around 79 FPS für simplenet und nur ca 10 für patchcore und ca 67 für DRAEM




\subsection{AST}
\label{subsec:AST}

\subsection{DRAEM}
\label{subsec:DRAEM}

\subsection{RevDist}
\label{subsec:revdist}

\subsection{CSFlow}
\label{subsec:csflow}





\input{algotable/sac}