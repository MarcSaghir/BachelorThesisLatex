\chapter{Background}
\label{chap:background}
This is an algorithm 



\section{Ensembles}
%TO-DO: irgendwo beispielhaft erklären dass manche approaches manche sachen besser verstehen und umgekehrt, deswegen ensemble gut ist, muss maybe in die background oder introduction

When it comes to ensembling classification models, there are multiple approaches to do so. Many ensembling methods are focussed on combining homogeneous 
models, meaning a set of related models with similar architecture but different parameters or initializations. Typical methods include 
(liste an methods mit referenzen, majority vote, boosting, bagging, stacking??, CAWPE, blabla), of which i.e. (ansatz für bäume ensembles) are a typical approach to boost simple 
classifiers like trees. Homogeneous ensembles are popular, since they tend to boost the performance and robustness of a base classifier without lots 
of additional work, since the ensemble is normally created by initializing the models in different ways.
%kann der teil über diesem kommentar in die introduction?
Heterogeneous classifier ensembles on the other hand are not necessarily combinable that easily, since they usually consist of models with 
different network architectures. This can lead to results, that should be interpreted as the same, differing by large margins(synonym). Yet 
ensembles of such variety are often desirable since they offer loads of information from different perspectives or domains when done right. 
% Kann der satz weg? Combining knowledge from different domains with such an ensemble is an effective way to improve a models performance in foreign domains.
Thus to bridge this gap at the output, a common approach is to first calibrate(referenz) and then ensemble each models 
output (beweis dafür dass das normal ist). For the last combination step, all ensemble techniques suited for homogeneous ensembling can 
be applied, due to the outputs being in a comparable state then. There are also approaches to collectively calibrate the hyperparameters 
of each heterogeneous classifier while classifying(referenz). While performance varies, combining 
these models in such a way is not necessarily regarded as the highest achievable robustness, especially when the classifiers work with features or some other form of inner representation. 
This stems from the fact that the model outputs are merely a small result of larger inner representations that may focus different aspects 
of information among the inputs. Therefore in turn, you cannot obtain all relevant information that can be offered by simply calibrating the 
model outputs. A more robust approach to address that problem, would be to ensemble the aforementioned inner representations, i.e. feature maps 
and in turn train another classifier for the final meaningful output.
%----nachfolgender satz maybe raus mal gucken oder anders einbauen
Another limitation of both kinds of ensembles, being homogeneous and heterogeneous, is that all models have to actually be trained 
seperately to then utilize the different classification ouputs. This leads to a highger training time and thus also higher computational 
cost, which is desirable to be reduced in real world manufacturing firms.
%-------
The robustness and efficiency has been demonstrated in \cite{EnsembleHeller2023} (ensemble referenz). The authors utilize a feature level ensemble of multiple 
convolutional neural networks with different architectures and tasks to improve inference speed and accuracy in plant disease detection.
(Heller referenz) show that cutting off several, potentially heterogeneous, classifiers after a couple of network layers and ensembling the 
resulting feature maps yields firstly a significant improvement in training time compared to classical output ensembles. This stems from 
the fact, that all base classifiers of the ensemble only have to be trained once for every following training approach. During this the 
model still stays compact(zitat aus ensemble paper markieren), giving it an advantage over most supervised approaches(satz klingt scheiße)(irgendwas mit 
lightweight hinschreiben? -> nochmal paper gucken). Moreover they compared the performance of different ensemble combinations with conventional 
output ensembles via softmax and reported in all cases no significant drop in performance. In cases where this approach allowed for 
different inputs via multispectral cameras(zitat markieren) there even was a similar performance of this ensemble to other state of the 
art ensembles visible. Keeping in mind the compactness of this new ensemble model combined with an equal performance and possible 
increased robustness, as argued prior, it is a promising ensemble approach for this work.
To obtain ensembled feature maps the paper proposes to bring all feature maps to the same sizes using bilinear interpolation. Since 
it is not desirable to keep every available feature map, as this would create inputs with way too many features, the amount of feature 
maps is reduced using principle component analysis. This allows for the ensemble to focus only on the most important features, while maintaining 
an equal amount of maps as if it were composed of a single classifier. To be more specific (Heller+co) \cite{EnsembleHeller2023} introduced two different 
approaches to perform this ensemble. The first is a global transformation block as seen in figure xyz(figure mit global transformation block). 
Here the features are first all resized to the same dimensions and then connected along their channels through a concatenation layer.
Afterwards PCA is applied along the channel dimension to obtain a result with N remaining feature maps, where N can be adjusted for ones 
needs.



- soll ich die transformation blocks erst bei den methods genauer beschreiben?

- hier vorgehensweise und findings von ensemble paper schreiben
- erster ansatz für vorgehensweise:
The actual combination of features from different level 1 classif


%part below maybe in methods
To obtain the different feature representations 
we would use the corresponding training methods of each IAD apporoach and then cut the model of at the respective time. Figures abc show a schematic view of each approachs respective model 
architecture, together with an indication of where the representations would be extracted. Proceeding in this way, we would keep all important features of each representation, resulting in a 
maxmium gain of information and robust predictions over all different classes.
Creating such heterogeneous model ensembles on a feature map level was for instance done in (paper ref). Among other results they investigate the performance of heterogeneous models being 
combined and provide two main approaches to doing so:
\textbf{General Transformation Block}




- talk about different ensemble approaches we discussed: ensemble model outputs and ensemble model feature maps

feature ensemble:
- ground idea: have different algos extract features, and then ensemble them. Afterwards train discriminator on the ensembled features like in simplenet
- reference paper that quses PCA and global block transformation
- global transformation block:
-> resize all feature maps to same dimensions
-> append feature maps
-> PCA: keep either percentage or set amount

- individual transformation block:
-> first apply PCA
-> sagen wann das am besten anwendbar ist, auch sagen dass für uns probably der global transformation block reicht
-> dann zusammenführen mit resize und appnden



\section{Classes of Anomaly detection}
When trying to understand the choices of IAD approaches for the pipeline and ensemble, one first has to learn about a few important distinctions of models on this topic.
The deep learning approaches that have established themselves as state of the art in image anomaly detection are almost exclusively unsupervised approaches. This partiall stems from the fact 
that naturally anomalous images occurr far less than normal images, hence the word "normal". This is especially true in industrial settings, due to the high performance of production factories 
nowadays. Therefore if one were to consider using a supervised learning approach to detect anomalies, either a strong class imbalance or an unrepresentative class distribution would occur.
While there are some solutions for this, they often are either not goo enough for imbalances this high(synonym klänge cool) or far to extensive. Some papers like (supervised papers zitieren)
utilize supervised approaches with some success, but still yield a worse performance than the popular unsupervised approaches generally used. Consequently the biggest model distinction is 
between unsupervised and supervised ones. Here it has to be said that there are technically also other settings of IAD one could talk about at this level of observation, but since we are also 
directing our focus to to RGB images, they will not be talked about. Moreover one has to make some simplifications to allow such sharp categorizations of partially interwoven approaches.

The supervised learning category could also further be split up into sub-categories at a lower level. But seeing as the performances of unsupervised approaches dominantely outweigh the 
performance and cost of the former, this work will solely focus on the latter kind of approaches. In the unsupervised IAD setting we then normally distinguish between reconstruction and 
representation based models. One of the key differences between those two is(hier dringend auch paper zitieren die das untersuchen), 


... 


If we now consider the classification of algorithms above, aswell as figure x, we can see that there are quite a lot of unique models and approaches to the same end. To ensure that the built 
pipeline is able to help experiment on images from different points of view, so to say, aswell as ensure that our ensemble approaches cover as various different aspects as possible, it is 
crucial to select approaches from majorly different branches. Here it may be noted that the performance of the single models is not completely disregarded, as those models may prove themselves 
not very useful in the ensemble setting or even as a point of view for experimentation. Therefore certain approaches from the survey papers ...., which yielded performances that were not 
remotely comparable with the highest performing models, were not considered, even if they might cover a previously unrepresented class of IAD setting. 
The main choices were:
- patchcore + paper
- DRAEM + paper
- CSFlow + paper

With this choice we still represent reconstruction and representation based settings somewhat comparably, aswell as providing different examples for a variety of subclasses, namely
distribution maps, autoencoder, memory banks, teacher-student models, diffusion models and ...



- there are different kinds of approaches to IAD
- look at tree picture
 
- First important distinction is between supervised and unsupervised
-> we focus on unsupervised
-> list problems with supervised approaches and thus advantages of unsupervised ones

- briefly touch on other IAD settings like few shot, along with references

- among unsupervised approaches, there are two more fundamental distinctions
-> reconstruction based vs representation/feature embedding based
-> explain difference with lots of references

- for reconstruction based touch on 2-3 base categories like GANs etc and link fundamental papers for GANs etc
- for representation based important to explain memory bank, teacher student, and distribution map
- explain normalizing flow somehow somewhere in there

- maybe say which algos we chose and what we covered with that



\section{Datasets}
The datasets used in image anomaly detection are scarce, especially when it comes to anomaly detection in a manufacturing setting. There are many datsets and approaches that specialize on certain materials \cite{FabricDataset_Tsang_2016} 
\cite{SteeltubeDataset_Yang_2021} \cite{magnetictiles_Huang_2018}
and often only one classs. What currently stands out as a gold standard among IAD datasets is the MVTecAD \cite{MVTEC_Bergmann_2021} dataset. The authors created it  
as a highly representative and standardized set of anomalous images along with training images. It has 15 classes from capsules to screws. Moreover the dataset provides image labels aswell as segmentation 
ground truths, making it versatile and applicable for multiple algorithms. The masks come as black and white grayscale images, while the image labels are given through its folder structure. 
Its paradigmatic structure tree can be seen in figure xy. As shown, each class contain train images, which only consist of regular examples, 
and test images. The data among the testing images is categorized by a title describing the anomaly. The ground truth folder contains 
according ground truths on a pixel level. Example images of the dataset are to be seen in figure z. They typically are of a rectangular shape and their resolutions range from 
(pixel min) to (pixel max). More specifications can be found in Bergmann et al. \cite{MVTEC_Bergmann_2021} and the whole dataset is publicly available at the official website\cite{mvtecdownload}.\newline
The MVTecAD(referenz) dataset is regarded highly among IAD papers, and has since its introduction been used in most relevant papers as a dataaset 
to benchmark the respective approaches on. This is also likely to remain the trend, since many state of the art algorithms in the recent years have primarily been benmarked on it, forcing new approaches 
to also be benchmarked on this dataset to be comparable to the current highest performance holding approaches. Despite this work focussing on manufacturing settings MVTecAD is one of only two datasets relevant to this work, 
and serves as a comparison for the performance investigation of this papers approaches on the second dataset. This is mainly due to the dataset's importance and 
its relation to the second dataset.
\newline
Later in 2022 Bergman et al. has introduced another IAD dataset that is loosely related to their original MVTecAD dataset, namely the MVTecAD LOCO dataset \cite{LOCODentsAndScratchesBergmann2022}. 
This dataset works with the same ground ideas as their original MVTecAD set, but extends the conceptual contents of the dataset by logical anomalies(neu formulieren das klingt scheiße). 
It consists of five classes: breakfast box, juice bottle, pushpins, screw bag and splicing connectors. The difference to the other dataset is that the anomalous categories for each class are only seperated into good images, images with structural anomalies 
and images with logical anomalies. As mentioned in the introduction structural anomalies are visible damages to the objects, similar to the MVTecAD dataset. Logical anomalies denote violations against arbitrary restrictions 
imposed by the authors. To illustrate this by an example: The class of pushpins represents a birds view of a compartmentised box of pushpins(see figure a). A rule added was, 
that each compartment is only to contain one pushpin. This means that if one region were to miss their contents, or contain more than one pushpin, it would constitute a logical anomaly. If on the 
other hand a pushpin would have a crooked or broken tip, it would be  labelled a structural anomaly. Structurally the differences of the 
MVTecAD and the MVTecAD LOCO dataset can be seen when comparing figures a and b, which showcases the anomaly classification, aswell as 
the method of storing segmentations. Here there eixists an image file for each anomalous ground truth area, which are mapped to the image 
by the folder name they are in. Lastly there exists a validation set in this dataset,
\newline
The addition of logical constraints opened an interesting area of research, since the high performance 
of current state of the art algorithms were only measured on structural anomalies so far. Yet it would be insightful to see if those models could also detect logical anomalies, since those also ocurr 
in real life settings, such as manufacturing settings. Another concept introduced in \cite{LOCODentsAndScratchesBergmann2022} is the 
saturated per-region overlap score, also sPRO. The metric is further analysed in setion (metrics section), but in short gives a measure 
on how well two regions overlap, while also accounting for regions overlapping in a way, that is seen as sufficient. The criterion of 
sufficiency is given by a file in the respective class, which maps a saturation score to each kind of anomaly.
Bergmann et al.\cite{LOCODentsAndScratchesBergmann2022} lastly also released a new IAD model together with the new dataset. The model uses autoencoders(bissi besser beschreiben hier). Since the source code has not been made public, 
this work refrains from using the method proposed in the paper.




\section{metrics}
Metrics are known to be an important part of developing any artificial intelligence related models. Many of them are used to infer 
different characteristics of model performance and should be used in different appropriate circumstances, depending on which aspect 
is important for the current application. Therefore, before the actual developing, one must first choose appropriate metrics 
to optimize and evaluate on later. IAD as a research area themselves has certain metrics that are the main performance evaluation tool 
across most papers. 
A collection of different metrics in this domain are displayed in table \ref{tab:metrics}, which is taken from 
\cite{liu2024deep}. Visible are well known 
ones from many other machine learning models like precision, recall, TPR, FPR and the F1-Score. These are generally applicable in most 
cases, but are not listed in any recent important papers and thus are not important for any analyses in this work. The other metrics are 
more IAD specific. The undisputed(synonym)/most popular scoring method is the AUROC. This metric is normally(synonym) used for image level 
binary classification and gives an indication on how good the model an distinguish between both classes. Its calculation can be seein in 
table \ref{tab:metrics} and it can also be used on a pixel level, which is done in some papers like (paper refernzen) but not everywhere.
Next in importance is the PRO score or also the area under the PRO score(AU-PRO). This metric denotes the per-region overlap of two areas 
on a pixel level and can be calculated using (PRO formel hier hin, im satz rechts dann ggf bezug auf formel zeichen nehmen). The two areas 
compared are generally an image mask and the according segmentation by the model. The AU-PRO is then calculated by plotting the PRO score 
at different threholds levels for the segmentations, and reporting the area under this curve. This can be 
used to rate the segmentation performance of different models and is also a metric featured frequently in IAD related research. 
Related to this score is the saturated per-region overlap (sPRO) and also the according are under the curve, the AU-sPRO. This metric was 
introduced in (bergmanns paper) \cite{LOCODentsAndScratchesBergmann2022}


- show metrics from survey papers
- some metrics are well known from other ML applications
- metrics that are important for our work/in most recent published papers are:
-> auroc: image/instance and pixel level
-> Area under PRO
-> explain formula

-> sPRO for LOCO and also Area under sPRO
--> Extra section where i explain sPRO on basis of dents and scratches paper
--> very detailed with saturation threshold and also include figure of comparison for PRO




\section{description of patchcore algo}


\section{description of simplenet}
- highlight the use of the discriminator because its important for mine

\section{description of AST}

\section{description of DRAEM}

\section{description of another reconstruction based algo}
















\input{algotable/sac}